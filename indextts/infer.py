import os
import sys
import time
from subprocess import CalledProcessError
from typing import Dict, List, Tuple

import torch
import torchaudio
from torch.nn.utils.rnn import pad_sequence
from omegaconf import OmegaConf
from tqdm import tqdm

import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

from indextts.BigVGAN.models import BigVGAN as Generator
from indextts.gpt.model import UnifiedVoice
from indextts.utils.checkpoint import load_checkpoint
from indextts.utils.feature_extractors import MelSpectrogramFeatures

from indextts.utils.front import TextNormalizer, TextTokenizer


class IndexTTS:
    def __init__(
        self, cfg_path="checkpoints/config.yaml", model_dir="checkpoints", is_fp16=True, device=None, use_cuda_kernel=None,
    ):
        """
        Args:
            cfg_path (str): path to the config file.
            model_dir (str): path to the model directory.
            is_fp16 (bool): whether to use fp16.
            device (str): device to use (e.g., 'cuda:0', 'cpu'). If None, it will be set automatically based on the availability of CUDA or MPS.
            use_cuda_kernel (None | bool): whether to use BigVGan custom fused activation CUDA kernel, only for CUDA device.
        """
        if device is not None:
            self.device = device
            self.is_fp16 = False if device == "cpu" else is_fp16
            self.use_cuda_kernel = use_cuda_kernel is not None and use_cuda_kernel and device.startswith("cuda")
        elif torch.cuda.is_available():
            self.device = "cuda:0"
            self.is_fp16 = is_fp16
            self.use_cuda_kernel = use_cuda_kernel is None or use_cuda_kernel
        elif hasattr(torch, "mps") and torch.backends.mps.is_available():
            self.device = "mps"
            self.is_fp16 = False # Use float16 on MPS is overhead than float32
            self.use_cuda_kernel = False
        else:
            self.device = "cpu"
            self.is_fp16 = False
            self.use_cuda_kernel = False
            print(">> Be patient, it may take a while to run in CPU mode.")

        self.cfg = OmegaConf.load(cfg_path)
        self.model_dir = model_dir
        self.dtype = torch.float16 if self.is_fp16 else None
        self.stop_mel_token = self.cfg.gpt.stop_mel_token

        # Comment-off to load the VQ-VAE model for debugging tokenizer
        #   https://github.com/index-tts/index-tts/issues/34
        #
        # from indextts.vqvae.xtts_dvae import DiscreteVAE
        # self.dvae = DiscreteVAE(**self.cfg.vqvae)
        # self.dvae_path = os.path.join(self.model_dir, self.cfg.dvae_checkpoint)
        # load_checkpoint(self.dvae, self.dvae_path)
        # self.dvae = self.dvae.to(self.device)
        # if self.is_fp16:
        #     self.dvae.eval().half()
        # else:
        #     self.dvae.eval()
        # print(">> vqvae weights restored from:", self.dvae_path)
        self.gpt = UnifiedVoice(**self.cfg.gpt)
        self.gpt_path = os.path.join(self.model_dir, self.cfg.gpt_checkpoint)
        load_checkpoint(self.gpt, self.gpt_path)
        self.gpt = self.gpt.to(self.device)
        if self.is_fp16:
            self.gpt.eval().half()
        else:
            self.gpt.eval()
        print(">> GPT weights restored from:", self.gpt_path)
        if self.is_fp16:
            try:
                import deepspeed

                use_deepspeed = True
            except (ImportError, OSError, CalledProcessError) as e:
                use_deepspeed = False
                print(f">> DeepSpeedÂä†ËΩΩÂ§±Ë¥•ÔºåÂõûÈÄÄÂà∞Ê†áÂáÜÊé®ÁêÜ: {e}")
                print("See more details https://www.deepspeed.ai/tutorials/advanced-install/")

            self.gpt.post_init_gpt2_config(use_deepspeed=use_deepspeed, kv_cache=True, half=True)
        else:
            self.gpt.post_init_gpt2_config(use_deepspeed=False, kv_cache=True, half=False)

        if self.use_cuda_kernel:
            # preload the CUDA kernel for BigVGAN
            try:
                from indextts.BigVGAN.alias_free_activation.cuda import load as anti_alias_activation_loader
                anti_alias_activation_cuda = anti_alias_activation_loader.load()
                print(">> Preload custom CUDA kernel for BigVGAN", anti_alias_activation_cuda)
            except Exception as e:
                print(">> Failed to load custom CUDA kernel for BigVGAN. Falling back to torch.", e, file=sys.stderr)
                print(" Reinstall with `pip install -e . --no-deps --no-build-isolation` to prebuild `anti_alias_activation_cuda` kernel.", file=sys.stderr)
                print(
                    "See more details: https://github.com/index-tts/index-tts/issues/164#issuecomment-2903453206", file=sys.stderr
                )
                self.use_cuda_kernel = False
        self.bigvgan = Generator(self.cfg.bigvgan, use_cuda_kernel=self.use_cuda_kernel)
        self.bigvgan_path = os.path.join(self.model_dir, self.cfg.bigvgan_checkpoint)
        vocoder_dict = torch.load(self.bigvgan_path, map_location="cpu")
        self.bigvgan.load_state_dict(vocoder_dict["generator"])
        self.bigvgan = self.bigvgan.to(self.device)
        # remove weight norm on eval mode
        self.bigvgan.remove_weight_norm()
        self.bigvgan.eval()
        print(">> bigvgan weights restored from:", self.bigvgan_path)
        self.bpe_path = os.path.join(self.model_dir, self.cfg.dataset["bpe_model"])
        self.normalizer = TextNormalizer()
        self.normalizer.load()
        print(">> TextNormalizer loaded")
        self.tokenizer = TextTokenizer(self.bpe_path, self.normalizer)
        print(">> bpe model loaded from:", self.bpe_path)
        # ÁºìÂ≠òÂèÇËÄÉÈü≥È¢ëmelÔºö
        self.cache_audio_prompt = None
        self.cache_cond_mel = None
        # ËøõÂ∫¶ÂºïÁî®ÊòæÁ§∫ÔºàÂèØÈÄâÔºâ
        self.gr_progress = None
        self.model_version = self.cfg.version if hasattr(self.cfg, "version") else None

    def remove_long_silence(self, codes: torch.Tensor, silent_token=52, max_consecutive=30):
        """
        Shrink special tokens (silent_token and stop_mel_token) in codes
        codes: [B, T]
        """
        code_lens = []
        codes_list = []
        device = codes.device
        dtype = codes.dtype
        isfix = False
        for i in range(0, codes.shape[0]):
            code = codes[i]
            if not torch.any(code == self.stop_mel_token).item():
                len_ = code.size(0)
            else:
                stop_mel_idx = (code == self.stop_mel_token).nonzero(as_tuple=False)
                len_ = stop_mel_idx[0].item() if len(stop_mel_idx) > 0 else code.size(0)

            count = torch.sum(code == silent_token).item()
            if count > max_consecutive:
                # code = code.cpu().tolist()
                ncode_idx = []
                n = 0
                for k in range(len_):
                    assert code[k] != self.stop_mel_token, f"stop_mel_token {self.stop_mel_token} should be shrinked here"
                    if code[k] != silent_token:
                        ncode_idx.append(k)
                        n = 0
                    elif code[k] == silent_token and n < 10:
                        ncode_idx.append(k)
                        n += 1
                    # if (k == 0 and code[k] == 52) or (code[k] == 52 and code[k-1] == 52):
                    #    n += 1
                # new code
                len_ = len(ncode_idx)
                codes_list.append(code[ncode_idx])
                isfix = True
            else:
                # shrink to len_
                codes_list.append(code[:len_])
            code_lens.append(len_)
        if isfix:
            if len(codes_list) > 1:
                codes = pad_sequence(codes_list, batch_first=True, padding_value=self.stop_mel_token)
            else:
                codes = codes_list[0].unsqueeze(0)
        else:
            # unchanged
            pass
        # clip codes to max length
        max_len = max(code_lens)
        if max_len < codes.shape[1]:
            codes = codes[:, :max_len]
        code_lens = torch.tensor(code_lens, dtype=torch.long, device=device)
        return codes, code_lens

    def bucket_sentences(self, sentences, bucket_max_size=4) -> List[List[Dict]]:
        """
        Sentence data bucketing.
        if ``bucket_max_size=1``, return all sentences in one bucket.
        """
        outputs: List[Dict] = []
        for idx, sent in enumerate(sentences):
            outputs.append({"idx": idx, "sent": sent, "len": len(sent)})
       
        if len(outputs) > bucket_max_size:
            # split sentences into buckets by sentence length
            buckets: List[List[Dict]] = []
            factor = 1.5
            last_bucket = None
            last_bucket_sent_len_median = 0

            for sent in sorted(outputs, key=lambda x: x["len"]):
                current_sent_len = sent["len"]
                if current_sent_len == 0:
                    print(">> skip empty sentence")
                    continue
                if last_bucket is None \
                        or current_sent_len >= int(last_bucket_sent_len_median * factor) \
                        or len(last_bucket) >= bucket_max_size:
                    # new bucket
                    buckets.append([sent])
                    last_bucket = buckets[-1]
                    last_bucket_sent_len_median = current_sent_len
                else:
                    # current bucket can hold more sentences
                    last_bucket.append(sent) # sorted
                    mid = len(last_bucket) // 2
                    last_bucket_sent_len_median = last_bucket[mid]["len"]
            last_bucket=None
            # merge all buckets with size 1
            out_buckets: List[List[Dict]] = []
            only_ones: List[Dict] = []
            for b in buckets:
                if len(b) == 1:
                    only_ones.append(b[0])
                else:
                    out_buckets.append(b)
            if len(only_ones) > 0:
                # merge into previous buckets if possible
                # print("only_ones:", [(o["idx"], o["len"]) for o in only_ones])
                for i in range(len(out_buckets)):
                    b = out_buckets[i]
                    if len(b) < bucket_max_size:
                        b.append(only_ones.pop(0))
                        if len(only_ones) == 0:
                            break
                # combined all remaining sized 1 buckets
                if len(only_ones) > 0:
                    out_buckets.extend([only_ones[i:i+bucket_max_size] for i in range(0, len(only_ones), bucket_max_size)])
            return out_buckets
        return [outputs]

    def pad_tokens_cat(self, tokens: List[torch.Tensor]) -> torch.Tensor:
        if self.model_version and self.model_version >= 1.5:
            # 1.5ÁâàÊú¨‰ª•‰∏äÔºåÁõ¥Êé•‰ΩøÁî®stop_text_token Âè≥‰æßÂ°´ÂÖÖÔºåÂ°´ÂÖÖÂà∞ÊúÄÂ§ßÈïøÂ∫¶
            # [1, N] -> [N,]
            tokens = [t.squeeze(0) for t in tokens]
            return pad_sequence(tokens, batch_first=True, padding_value=self.cfg.gpt.stop_text_token, padding_side="right")
        max_len = max(t.size(1) for t in tokens)
        outputs = []
        for tensor in tokens:
            pad_len = max_len - tensor.size(1)
            if pad_len > 0:
                n = min(8, pad_len)
                tensor = torch.nn.functional.pad(tensor, (0, n), value=self.cfg.gpt.stop_text_token)
                tensor = torch.nn.functional.pad(tensor, (0, pad_len - n), value=self.cfg.gpt.start_text_token)
            tensor = tensor[:, :max_len]
            outputs.append(tensor)
        tokens = torch.cat(outputs, dim=0)
        return tokens

    def torch_empty_cache(self, verbose=False):
        try:
            if "cuda" in str(self.device):
                if verbose:
                    # ÊâìÂç∞ÂÜÖÂ≠ò‰ΩøÁî®ÊÉÖÂÜµÔºàË∞ÉËØïÁî®Ôºâ
                    memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3  # GB
                    memory_cached = torch.cuda.memory_reserved(self.device) / 1024**3  # GB
                    print(f">> GPU Memory before cleanup: Allocated={memory_allocated:.2f}GB, Cached={memory_cached:.2f}GB")
                torch.cuda.empty_cache()
                if verbose:
                    memory_allocated_after = torch.cuda.memory_allocated(self.device) / 1024**3  # GB
                    memory_cached_after = torch.cuda.memory_reserved(self.device) / 1024**3  # GB
                    print(f">> GPU Memory after cleanup: Allocated={memory_allocated_after:.2f}GB, Cached={memory_cached_after:.2f}GB")
            elif "mps" in str(self.device):
                torch.mps.empty_cache()
        except Exception as e:
            if verbose:
                print(f">> Warning: Failed to clear cache: {e}")

    def comprehensive_memory_cleanup(self, verbose=False):
        """
        ÊâßË°åÂÖ®Èù¢ÁöÑÂÜÖÂ≠òÊ∏ÖÁêÜÔºåÂåÖÊã¨Ê®°ÂûãÁºìÂ≠ò„ÄÅKVÁºìÂ≠òÁ≠â
        """
        try:
            if verbose:
                memory_before = torch.cuda.memory_allocated(self.device) / 1024**3 if "cuda" in str(self.device) else 0
                print(f">> ÂºÄÂßãÂÖ®Èù¢ÂÜÖÂ≠òÊ∏ÖÁêÜÔºåÂΩìÂâçÂç†Áî®: {memory_before:.2f}GB")
            
            # Ê∏ÖÁêÜGPTÊ®°ÂûãÁöÑKVÁºìÂ≠ò
            if hasattr(self.gpt, 'inference_model') and hasattr(self.gpt.inference_model, 'cached_mel_emb'):
                self.gpt.inference_model.cached_mel_emb = None
                if verbose:
                    print(">> Â∑≤Ê∏ÖÁêÜGPTÊ®°ÂûãÁöÑcached_mel_emb")
            
            # Â¶ÇÊûúÊúâpast_key_valuesÁºìÂ≠òÔºå‰πüÊ∏ÖÁêÜÊéâ
            if hasattr(self.gpt, 'inference_model'):
                # Â∞ùËØïÊ∏ÖÁêÜtransformerÁöÑÁºìÂ≠ò
                for module in self.gpt.inference_model.modules():
                    if hasattr(module, 'past_key_values'):
                        module.past_key_values = None
                    if hasattr(module, '_cache'):
                        module._cache = None
            
            # Ê∏ÖÁêÜBigVGANÊ®°ÂûãÁöÑÊΩúÂú®ÁºìÂ≠ò
            if hasattr(self.bigvgan, '_cache'):
                self.bigvgan._cache = None
                
            # Ê∏ÖÁêÜÊù°‰ª∂Èü≥È¢ëÁºìÂ≠òÔºàÂèØÈÄâÔºåÊ†πÊçÆÈúÄË¶ÅÔºâ
            # Ê≥®ÊÑèÔºöËøô‰ºöÂØºËá¥‰∏ãÊ¨°‰ΩøÁî®Áõ∏ÂêåÈü≥È¢ëÊó∂ÈáçÊñ∞ËÆ°ÁÆó
            # self.cache_cond_mel = None
            # self.cache_audio_prompt = None
            
            # ÊâßË°åPyTorchÁöÑÂÜÖÂ≠òÊ∏ÖÁêÜ
            self.torch_empty_cache(verbose=False)
            
            # Âº∫Âà∂PythonÂûÉÂúæÂõûÊî∂
            import gc
            gc.collect()
            
            if verbose:
                memory_after = torch.cuda.memory_allocated(self.device) / 1024**3 if "cuda" in str(self.device) else 0
                freed = memory_before - memory_after
                print(f">> ÂÖ®Èù¢ÂÜÖÂ≠òÊ∏ÖÁêÜÂÆåÊàêÔºåÈáäÊîæ: {freed:.2f}GBÔºåÂΩìÂâçÂç†Áî®: {memory_after:.2f}GB")
                
        except Exception as e:
            if verbose:
                print(f">> Warning: ÂÖ®Èù¢ÂÜÖÂ≠òÊ∏ÖÁêÜÊó∂ÂèëÁîüÈîôËØØ: {e}")

    def format_time(self, seconds):
        """Ê†ºÂºèÂåñÊó∂Èó¥‰∏∫‰∫∫Á±ªÂèØËØªÊ†ºÂºè"""
        if seconds < 60:
            return f"{int(seconds)}Áßí"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = int(seconds % 60)
            return f"{minutes}ÂàÜ{secs}Áßí"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            return f"{hours}Â∞èÊó∂{minutes}ÂàÜÈíü"
    
    def get_system_info(self, force_refresh=True):
        """Ëé∑ÂèñÁ≥ªÁªü‰ø°ÊÅØÂåÖÊã¨ÊòæÂ≠ò„ÄÅÂÜÖÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ"""
        import psutil
        import os
        
        system_info = {}
        
        # GPU‰ø°ÊÅØ - Âº∫Âà∂Âà∑Êñ∞Á°Æ‰øùÂÆûÊó∂ÊÄß
        if "cuda" in str(self.device):
            try:
                import torch
                # Â§öÈáçÂº∫Âà∂ÂêåÊ≠•Á°Æ‰øùËé∑ÂèñÊúÄÊñ∞ÁöÑÊòæÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ
                if force_refresh:
                    torch.cuda.synchronize(self.device)
                    torch.cuda.empty_cache()  # Ê∏ÖÁêÜÁºìÂ≠òÁ°Æ‰øùÂáÜÁ°ÆÊÄß
                    torch.cuda.synchronize(self.device)  # ÂÜçÊ¨°ÂêåÊ≠•
                
                # Ëé∑ÂèñÁúüÂÆûÁöÑGPUÊòæÂ≠ò‰ΩøÁî®ÊÉÖÂÜµ
                gpu_memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3  # GB
                gpu_memory_reserved = torch.cuda.memory_reserved(self.device) / 1024**3  # GB
                gpu_memory_total = torch.cuda.get_device_properties(self.device).total_memory / 1024**3  # GB
                
                # ËÆ°ÁÆóÂÆûÈôÖ‰ΩøÁî®ÁéáÔºàÂü∫‰∫éÂ∑≤ÂàÜÈÖçÂÜÖÂ≠òÔºâ
                gpu_usage_percent = (gpu_memory_allocated / gpu_memory_total) * 100
                
                system_info.update({
                    "gpu_memory_allocated": gpu_memory_allocated,
                    "gpu_memory_reserved": gpu_memory_reserved,
                    "gpu_memory_total": gpu_memory_total,
                    "gpu_memory_usage_percent": gpu_usage_percent,
                    "gpu_name": torch.cuda.get_device_name(self.device),
                    "gpu_device_index": self.device.index if hasattr(self.device, 'index') else 0
                })
                
                # Ê∑ªÂä†ËØ¶ÁªÜÁöÑÊòæÂ≠òÁªüËÆ°ÔºàÁî®‰∫éË∞ÉËØïÔºâ
                if force_refresh:
                    memory_summary = torch.cuda.memory_summary(self.device)
                    system_info["gpu_memory_summary"] = memory_summary
                    
            except Exception as e:
                system_info["gpu_error"] = str(e)
                print(f"GPU‰ø°ÊÅØËé∑ÂèñÈîôËØØ: {e}")
        
        # CPUÂíåÂÜÖÂ≠ò‰ø°ÊÅØ
        try:
            # ‰ΩøÁî®ËæÉÁü≠ÁöÑintervalÁ°Æ‰øùÂÆûÊó∂ÊÄß
            cpu_percent = psutil.cpu_percent(interval=0.01)  # Êõ¥Áü≠ÁöÑinterval
            memory = psutil.virtual_memory()
            
            system_info.update({
                "cpu_percent": cpu_percent,
                "memory_used": memory.used / 1024**3,  # GB
                "memory_total": memory.total / 1024**3,  # GB
                "memory_percent": memory.percent,
                "process_memory": psutil.Process(os.getpid()).memory_info().rss / 1024**3  # GB
            })
        except Exception as e:
            system_info["system_error"] = str(e)
            
        return system_info

    def _calculate_smart_eta(self, elapsed_time, current_item, total_items, batch_times=None):
        """Êô∫ËÉΩETAËÆ°ÁÆóÁÆóÊ≥ïËûçÂêàÂ§öÁßçÈ¢ÑÊµãÊñπÊ≥ï"""
        if current_item <= 0 or total_items <= 0:
            return None, "Êï∞ÊçÆ‰∏çË∂≥"
            
        progress_percent = current_item / total_items
        remaining_items = total_items - current_item
        
        predictions = []
        confidence_scores = []
        
        # 1. Á∫øÊÄßÈ¢ÑÊµãÔºàÂü∫Á°ÄÈ¢ÑÊµãÔºâ
        linear_eta = (elapsed_time / current_item) * remaining_items
        predictions.append(linear_eta)
        confidence_scores.append(0.3)  # Âü∫Á°ÄÊùÉÈáç
        
        # 2. ÊâπÊ¨°Êó∂Èó¥È¢ÑÊµãÔºàÂ¶ÇÊûúÊúâÊâπÊ¨°Êï∞ÊçÆÔºâ
        if batch_times and len(batch_times) > 0:
            # ËøáÊª§ÂºÇÂ∏∏ÂÄºÔºàË∂ÖËøá‰∏≠‰ΩçÊï∞2ÂÄçÁöÑÊó∂Èó¥Ôºâ
            filtered_times = self._filter_outliers(batch_times)
            
            if len(filtered_times) >= 2:
                # ÊåáÊï∞Âä†ÊùÉÁßªÂä®Âπ≥ÂùáÔºàEWMAÔºâ
                alpha = 0.3  # Âπ≥ÊªëÁ≥ªÊï∞
                ewma_time = filtered_times[0]
                for t in filtered_times[1:]:
                    ewma_time = alpha * t + (1 - alpha) * ewma_time
                
                batch_eta = ewma_time * remaining_items
                predictions.append(batch_eta)
                confidence_scores.append(min(0.7, len(filtered_times) * 0.1))  # Ê†πÊçÆÊï∞ÊçÆÈáèË∞ÉÊï¥ÊùÉÈáç
                
                # 3. Ë∂ãÂäøÈ¢ÑÊµãÔºàÂü∫‰∫éÊúÄËøëË∂ãÂäøÔºâ
                if len(filtered_times) >= 3:
                    recent_times = filtered_times[-3:]
                    if len(recent_times) >= 2:
                        # ËÆ°ÁÆóË∂ãÂäøÊñúÁéá
                        x = list(range(len(recent_times)))
                        y = recent_times
                        
                        # ÁÆÄÂçïÁ∫øÊÄßÂõûÂΩí
                        n = len(x)
                        sum_x = sum(x)
                        sum_y = sum(y)
                        sum_xy = sum(x[i] * y[i] for i in range(n))
                        sum_x2 = sum(x[i] * x[i] for i in range(n))
                        
                        if n * sum_x2 - sum_x * sum_x != 0:
                            slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
                            intercept = (sum_y - slope * sum_x) / n
                            
                            # È¢ÑÊµã‰∏ã‰∏Ä‰∏™ÊâπÊ¨°Êó∂Èó¥
                            next_batch_time = slope * len(recent_times) + intercept
                            next_batch_time = max(0.1, next_batch_time)  # Èò≤Ê≠¢Ë¥üÂÄºÊàñËøáÂ∞èÂÄº
                            
                            trend_eta = next_batch_time * remaining_items
                            predictions.append(trend_eta)
                            confidence_scores.append(0.4)
        
        # 4. Âü∫‰∫éËøõÂ∫¶Èò∂ÊÆµÁöÑË∞ÉÊï¥
        stage_multiplier = 1.0
        if progress_percent < 0.1:
            # ÂàùÂßãÈò∂ÊÆµÔºåÊó∂Èó¥ÂèØËÉΩ‰∏çÁ®≥ÂÆö
            stage_multiplier = 1.2
        elif progress_percent > 0.8:
            # Êé•ËøëÂÆåÊàêÔºåÈÄöÂ∏∏‰ºöÂä†ÈÄü
            stage_multiplier = 0.9
        
        # 5. ËÆ°ÁÆóÂä†ÊùÉÂπ≥Âùá
        if len(predictions) > 0:
            total_weight = sum(confidence_scores)
            if total_weight > 0:
                weighted_eta = sum(pred * weight for pred, weight in zip(predictions, confidence_scores)) / total_weight
            else:
                weighted_eta = predictions[0]
                
            # Â∫îÁî®Èò∂ÊÆµË∞ÉÊï¥
            weighted_eta *= stage_multiplier
            
            # 6. Âπ≥ÊªëÂ§ÑÁêÜÔºåÈÅøÂÖçÂâßÁÉàÊ≥¢Âä®
            if hasattr(self, '_last_eta') and self._last_eta is not None:
                # ‰ΩøÁî®ÊåáÊï∞Âπ≥Êªë
                smooth_factor = 0.7  # Âπ≥ÊªëÁ≥ªÊï∞
                weighted_eta = smooth_factor * weighted_eta + (1 - smooth_factor) * self._last_eta
            
            self._last_eta = weighted_eta
            
            # ÁîüÊàêÁΩÆ‰ø°Â∫¶ÊèèËø∞
            confidence_level = min(total_weight, 1.0)
            if confidence_level > 0.8:
                confidence_desc = "È´òÁ≤æÂ∫¶"
            elif confidence_level > 0.5:
                confidence_desc = "‰∏≠Á≠âÁ≤æÂ∫¶"
            else:
                confidence_desc = "È¢Ñ‰º∞"
                
            return weighted_eta, confidence_desc
        
        return linear_eta, "Âü∫Á°ÄÈ¢Ñ‰º∞"
    
    def _filter_outliers(self, times, threshold=2.0):
        """ËøáÊª§ÂºÇÂ∏∏ÂÄº"""
        if len(times) < 3:
            return times
            
        # ËÆ°ÁÆó‰∏≠‰ΩçÊï∞Âíå‰∏≠‰ΩçÊï∞ÁªùÂØπÂÅèÂ∑Æ
        sorted_times = sorted(times)
        median = sorted_times[len(sorted_times) // 2]
        
        # ËøáÊª§Ë∂ÖËøáÈòàÂÄºÁöÑÂÄº
        filtered = []
        for t in times:
            if t <= median * threshold and t >= median / threshold:
                filtered.append(t)
        
        # Â¶ÇÊûúËøáÊª§ÂêéÊï∞ÊçÆÂ§™Â∞ëÔºå‰øùÁïôÂéüÂßãÊï∞ÊçÆ
        return filtered if len(filtered) >= len(times) // 2 else times
    
    def _get_speed_info(self, batch_times, current_item, elapsed_time):
        """Ëé∑ÂèñÈÄüÂ∫¶‰ø°ÊÅØ"""
        if not batch_times or len(batch_times) == 0:
            return ""
            
        # ÂΩìÂâçÈÄüÂ∫¶
        if len(batch_times) > 0:
            current_speed = 1.0 / batch_times[-1] if batch_times[-1] > 0 else 0
            current_speed_text = f"{current_speed:.1f}/s"
        else:
            current_speed_text = "ËÆ°ÁÆó‰∏≠"
            
        # Âπ≥ÂùáÈÄüÂ∫¶
        if elapsed_time > 0 and current_item > 0:
            avg_speed = current_item / elapsed_time
            avg_speed_text = f"{avg_speed:.1f}/s"
        else:
            avg_speed_text = "ËÆ°ÁÆó‰∏≠"
            
        return f"\n‚ö° Â§ÑÁêÜÈÄüÂ∫¶: ÂΩìÂâç {current_speed_text} | Âπ≥Âùá {avg_speed_text}"

    def _set_gr_progress(self, value, desc, start_time=None, total_items=None, current_item=None, batch_times=None):
        """Êô∫ËÉΩËøõÂ∫¶Êõ¥Êñ∞ÔºåÂåÖÂê´È´òÁ∫ßÊó∂Èó¥‰º∞ÁÆóÂíåÁ≥ªÁªü‰ø°ÊÅØ"""
        # Ëé∑ÂèñÂÆûÊó∂Á≥ªÁªü‰ø°ÊÅØ
        system_info = self.get_system_info(force_refresh=True)
        
        # Êó∂Èó¥ËÆ°ÁÆó
        time_info = ""
        confidence_desc = ""
        
        if start_time is not None:
            elapsed_time = time.perf_counter() - start_time
            elapsed_formatted = self.format_time(elapsed_time)
            
            if total_items and current_item and current_item > 0:
                # ‰ΩøÁî®Êô∫ËÉΩETAÁÆóÊ≥ï
                estimated_remaining, confidence = self._calculate_smart_eta(
                    elapsed_time, current_item, total_items, batch_times
                )
                
                if estimated_remaining is not None:
                    remaining_formatted = self.format_time(estimated_remaining)
                    time_info = f"\n‚è±Ô∏è Â∑≤Áî®Êó∂: {elapsed_formatted} | È¢ÑËÆ°Ââ©‰Ωô: {remaining_formatted} ({confidence})"
                    confidence_desc = confidence
                    
                    # Ê∑ªÂä†ÈÄüÂ∫¶‰ø°ÊÅØ
                    speed_info = self._get_speed_info(batch_times, current_item, elapsed_time)
                    time_info += speed_info
                    
                    # Ê∑ªÂä†ÊâπÊ¨°ËØ¶ÊÉÖÔºàÂ¶ÇÊûúÊúâÊâπÊ¨°Êï∞ÊçÆÔºâ
                    if batch_times and len(batch_times) > 1:
                        last_batch_time = batch_times[-1]
                        filtered_times = self._filter_outliers(batch_times)
                        avg_batch_time = sum(filtered_times) / len(filtered_times)
                        
                        last_batch_formatted = self.format_time(last_batch_time)
                        avg_batch_formatted = self.format_time(avg_batch_time)
                        time_info += f"\nüìä ÂΩìÂâçÊâπÊ¨°: {last_batch_formatted} | Âπ≥ÂùáÊâπÊ¨°: {avg_batch_formatted}"
                        
                        # ÊòæÁ§∫ËøõÂ∫¶ÁôæÂàÜÊØîÂíåÂâ©‰ΩôÈ°πÁõÆ
                        progress_percent = (current_item / total_items) * 100
                        time_info += f"\nüìà ËøõÂ∫¶: {current_item}/{total_items} ({progress_percent:.1f}%)"
                else:
                    time_info = f"\n‚è±Ô∏è Â∑≤Áî®Êó∂: {elapsed_formatted} | È¢ÑËÆ°Ââ©‰Ωô: ËÆ°ÁÆó‰∏≠..."
            else:
                time_info = f"\n‚è±Ô∏è Â∑≤Áî®Êó∂: {elapsed_formatted}"
        
        # ÊûÑÂª∫Á≥ªÁªü‰ø°ÊÅØÂ≠óÁ¨¶‰∏≤ÔºàÁßªÈô§GPU‰ø°ÊÅØÔºâ
        sys_info = ""
        
        if "memory_percent" in system_info:
            mem_percent = system_info["memory_percent"]
            mem_used = system_info["memory_used"]
            mem_total = system_info["memory_total"]
            process_mem = system_info["process_memory"]
            sys_info += f"\nüíæ Á≥ªÁªüÂÜÖÂ≠ò: {mem_used:.1f}/{mem_total:.1f}GB ({mem_percent:.1f}%) | ËøõÁ®ã: {process_mem:.1f}GB"
        
        if "cpu_percent" in system_info:
            cpu_percent = system_info["cpu_percent"]
            sys_info += f"\nüñ•Ô∏è CPU: {cpu_percent:.1f}%"
        
        # ÂÆåÊï¥ÁöÑÊèèËø∞‰ø°ÊÅØ
        full_desc = desc + time_info + sys_info
        
        if self.gr_progress is not None:
            self.gr_progress(value, desc=full_desc)
        
        # ÊéßÂà∂Âè∞ËæìÂá∫ÔºàÊô∫ËÉΩÁâàÊú¨Ôºâ
        console_msg = f">> ËøõÂ∫¶ {value*100:.1f}%: {desc}"
        
        # Ê∑ªÂä†Êó∂Èó¥‰ø°ÊÅØ
        if start_time:
            elapsed = time.perf_counter() - start_time
            console_msg += f" (Â∑≤Áî®Êó∂: {self.format_time(elapsed)}"
            
            # Ê∑ªÂä†Êô∫ËÉΩÈ¢ÑËÆ°Ââ©‰ΩôÊó∂Èó¥
            if total_items and current_item and current_item > 0:
                estimated_remaining, confidence = self._calculate_smart_eta(
                    elapsed, current_item, total_items, batch_times
                )
                
                if estimated_remaining is not None:
                    remaining_formatted = self.format_time(estimated_remaining)
                    console_msg += f", È¢ÑËÆ°Ââ©‰Ωô: {remaining_formatted}"
                    if confidence != "Âü∫Á°ÄÈ¢Ñ‰º∞":
                        console_msg += f" [{confidence}]"
                else:
                    console_msg += ", È¢ÑËÆ°Ââ©‰Ωô: ËÆ°ÁÆó‰∏≠"
            console_msg += ")"
        
        print(console_msg)

    # Âø´ÈÄüÊé®ÁêÜÔºöÂØπ‰∫é"Â§öÂè•ÈïøÊñáÊú¨"ÔºåÂèØÂÆûÁé∞Ëá≥Â∞ë 2~10 ÂÄç‰ª•‰∏äÁöÑÈÄüÂ∫¶ÊèêÂçá~ ÔºàFirst modified by sunnyboxs 2025-04-16Ôºâ
    def infer_fast(self, audio_prompt, text, output_path, verbose=False, max_text_tokens_per_sentence=100, sentences_bucket_max_size=4, **generation_kwargs):
        """
        Args:
            ``max_text_tokens_per_sentence``: ÂàÜÂè•ÁöÑÊúÄÂ§ßtokenÊï∞ÔºåÈªòËÆ§``100``ÔºåÂèØ‰ª•Ê†πÊçÆGPUÁ°¨‰ª∂ÊÉÖÂÜµË∞ÉÊï¥
                - Ë∂äÂ∞èÔºåbatch Ë∂äÂ§öÔºåÊé®ÁêÜÈÄüÂ∫¶Ë∂ä*Âø´*ÔºåÂç†Áî®ÂÜÖÂ≠òÊõ¥Â§öÔºåÂèØËÉΩÂΩ±ÂìçË¥®Èáè
                - Ë∂äÂ§ßÔºåbatch Ë∂äÂ∞ëÔºåÊé®ÁêÜÈÄüÂ∫¶Ë∂ä*ÊÖ¢*ÔºåÂç†Áî®ÂÜÖÂ≠òÂíåË¥®ÈáèÊõ¥Êé•Ëøë‰∫éÈùûÂø´ÈÄüÊé®ÁêÜ
            ``sentences_bucket_max_size``: ÂàÜÂè•ÂàÜÊ°∂ÁöÑÊúÄÂ§ßÂÆπÈáèÔºåÈªòËÆ§``4``ÔºåÂèØ‰ª•Ê†πÊçÆGPUÂÜÖÂ≠òË∞ÉÊï¥
                - Ë∂äÂ§ßÔºåbucketÊï∞ÈáèË∂äÂ∞ëÔºåbatchË∂äÂ§öÔºåÊé®ÁêÜÈÄüÂ∫¶Ë∂ä*Âø´*ÔºåÂç†Áî®ÂÜÖÂ≠òÊõ¥Â§öÔºåÂèØËÉΩÂΩ±ÂìçË¥®Èáè
                - Ë∂äÂ∞èÔºåbucketÊï∞ÈáèË∂äÂ§öÔºåbatchË∂äÂ∞ëÔºåÊé®ÁêÜÈÄüÂ∫¶Ë∂ä*ÊÖ¢*ÔºåÂç†Áî®ÂÜÖÂ≠òÂíåË¥®ÈáèÊõ¥Êé•Ëøë‰∫éÈùûÂø´ÈÄüÊé®ÁêÜ
        """
        print(">> start fast inference...")
        
        # ÂºÄÂßãÊó∂Ê∏ÖÁêÜÁºìÂ≠òÂπ∂ÁõëÊéßÂÜÖÂ≠ò
        self.comprehensive_memory_cleanup(verbose=verbose)
        
        start_time = time.perf_counter()
        self._set_gr_progress(0, "start fast inference...", start_time=start_time)
        if verbose:
            print(f"origin text:{text}")

        # Â¶ÇÊûúÂèÇËÄÉÈü≥È¢ëÊîπÂèò‰∫ÜÔºåÊâçÈúÄË¶ÅÈáçÊñ∞ÁîüÊàê cond_mel, ÊèêÂçáÈÄüÂ∫¶
        if self.cache_cond_mel is None or self.cache_audio_prompt != audio_prompt:
            audio, sr = torchaudio.load(audio_prompt)
            audio = torch.mean(audio, dim=0, keepdim=True)
            if audio.shape[0] > 1:
                audio = audio[0].unsqueeze(0)
            audio = torchaudio.transforms.Resample(sr, 24000)(audio)
            cond_mel = MelSpectrogramFeatures()(audio).to(self.device)
            cond_mel_frame = cond_mel.shape[-1]
            if verbose:
                print(f"cond_mel shape: {cond_mel.shape}", "dtype:", cond_mel.dtype)

            self.cache_audio_prompt = audio_prompt
            self.cache_cond_mel = cond_mel
        else:
            cond_mel = self.cache_cond_mel
            cond_mel_frame = cond_mel.shape[-1]
            pass

        auto_conditioning = cond_mel
        cond_mel_lengths = torch.tensor([cond_mel_frame], device=self.device)

        # text_tokens
        text_tokens_list = self.tokenizer.tokenize(text)

        sentences = self.tokenizer.split_sentences(text_tokens_list, max_tokens_per_sentence=max_text_tokens_per_sentence)
        if verbose:
            print(">> text token count:", len(text_tokens_list))
            print("   splited sentences count:", len(sentences))
            print("   max_text_tokens_per_sentence:", max_text_tokens_per_sentence)
            print(*sentences, sep="\n")
        do_sample = generation_kwargs.pop("do_sample", True)
        top_p = generation_kwargs.pop("top_p", 0.8)
        top_k = generation_kwargs.pop("top_k", 30)
        temperature = generation_kwargs.pop("temperature", 1.0)
        autoregressive_batch_size = 1
        length_penalty = generation_kwargs.pop("length_penalty", 0.0)
        num_beams = generation_kwargs.pop("num_beams", 3)
        repetition_penalty = generation_kwargs.pop("repetition_penalty", 10.0)
        max_mel_tokens = generation_kwargs.pop("max_mel_tokens", 600)
        sampling_rate = 24000
        
        # ÊîπËøõÔºö‰ΩøÁî®ÊµÅÂºèÂ§ÑÁêÜÈÅøÂÖçÂ§ßÈáèÂÜÖÂ≠òÁ¥ØÁßØ
        wavs = []
        gpt_gen_time = 0
        gpt_forward_time = 0
        bigvgan_time = 0

        # text processing
        all_text_tokens: List[List[torch.Tensor]] = []
        self._set_gr_progress(0.1, "text processing...", start_time=start_time)
        bucket_max_size = sentences_bucket_max_size if self.device != "cpu" else 1
        all_sentences = self.bucket_sentences(sentences, bucket_max_size=bucket_max_size)
        bucket_count = len(all_sentences)
        if verbose:
            print(">> sentences bucket_count:", bucket_count,
                  "bucket sizes:", [(len(s), [t["idx"] for t in s]) for s in all_sentences],
                  "bucket_max_size:", bucket_max_size)
        
        # ËØ¶ÁªÜÁöÑÂàÜÂè•‰ø°ÊÅØ
        total_sentences = len(sentences)
        print(f">> ÊñáÊú¨ÂàÜÊûêÂÆåÊàê: {total_sentences} ‰∏™Âè•Â≠ê, {bucket_count} ‰∏™ÊâπÊ¨°")
        self._set_gr_progress(0.15, f"ÊñáÊú¨ÂàÜÊûêÂÆåÊàê: {total_sentences} ‰∏™Âè•Â≠ê, {bucket_count} ‰∏™ÊâπÊ¨°", start_time=start_time)
        for sentences in all_sentences:
            temp_tokens: List[torch.Tensor] = []
            all_text_tokens.append(temp_tokens)
            for item in sentences:
                sent = item["sent"]
                text_tokens = self.tokenizer.convert_tokens_to_ids(sent)
                text_tokens = torch.tensor(text_tokens, dtype=torch.int32, device=self.device).unsqueeze(0)
                if verbose:
                    print(text_tokens)
                    print(f"text_tokens shape: {text_tokens.shape}, text_tokens type: {text_tokens.dtype}")
                    # debug tokenizer
                    text_token_syms = self.tokenizer.convert_ids_to_tokens(text_tokens[0].tolist())
                    print("text_token_syms is same as sentence tokens", text_token_syms == sent) 
                temp_tokens.append(text_tokens)
        
        # ÊîπËøõÁöÑÂ§ÑÁêÜÁ≠ñÁï•ÔºöÊµÅÂºèÂ§ÑÁêÜÔºåÈÅøÂÖçÂ§ßÈáèÂÜÖÂ≠òÁ¥ØÁßØ
        print(">> ÂºÄÂßãÊµÅÂºèÊâπÊ¨°Â§ÑÁêÜ...")
        self._set_gr_progress(0.2, "ÂºÄÂßãÊµÅÂºèÊâπÊ¨°Â§ÑÁêÜ...", start_time=start_time)
        
        all_batch_num = sum(len(s) for s in all_sentences)
        processed_num = 0
        batch_idx = 0
        
        # ÊâπÊ¨°Êó∂Èó¥Ë∑üË∏™
        batch_times = []  # ËÆ∞ÂΩïÊØè‰∏™ÊâπÊ¨°ÁöÑÂ§ÑÁêÜÊó∂Èó¥
        batch_start_time = time.perf_counter()
        
        # Áî®‰∫éÊåâÈ°∫Â∫èÊî∂ÈõÜÊúÄÁªàÁöÑÈü≥È¢ëÁâáÊÆµ
        ordered_wavs = {}  # {original_idx: wav_tensor}
        
        # ÈÄêÊâπÊ¨°Â§ÑÁêÜÔºåÈÅøÂÖçÂÜÖÂ≠òÁ¥ØÁßØ
        for batch_sentences, item_tokens in zip(all_sentences, all_text_tokens):
            # ËÆ∞ÂΩïÂΩìÂâçÊâπÊ¨°ÂºÄÂßãÊó∂Èó¥
            current_batch_start = time.perf_counter()
            
            batch_num = len(item_tokens)
            if batch_num > 1:
                batch_text_tokens = self.pad_tokens_cat(item_tokens)
            else:
                batch_text_tokens = item_tokens[0]
            processed_num += batch_num
            
            # GPTÊé®ÁêÜÈò∂ÊÆµ
            progress_percent = 0.2 + 0.6 * processed_num/all_batch_num
            self._set_gr_progress(progress_percent, f"Â§ÑÁêÜÊâπÊ¨° {batch_idx+1}/{bucket_count} - Â§ßÂ∞è: {batch_num}", 
                                start_time=start_time, total_items=bucket_count, current_item=batch_idx, batch_times=batch_times)
            
            m_start_time = time.perf_counter()
            with torch.no_grad():
                with torch.amp.autocast(batch_text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    batch_codes = self.gpt.inference_speech(auto_conditioning, batch_text_tokens,
                                        cond_mel_lengths=cond_mel_lengths,
                                        do_sample=do_sample,
                                        top_p=top_p,
                                        top_k=top_k,
                                        temperature=temperature,
                                        num_return_sequences=autoregressive_batch_size,
                                        length_penalty=length_penalty,
                                        num_beams=num_beams,
                                        repetition_penalty=repetition_penalty,
                                        max_generate_length=max_mel_tokens,
                                        **generation_kwargs)
            gpt_gen_time += time.perf_counter() - m_start_time
            
            # Á´ãÂç≥Â§ÑÁêÜÂΩìÂâçÊâπÊ¨°ÁöÑlatentsÂíåÈü≥È¢ëÁîüÊàê
            batch_wavs = []
            has_warned = False
            
            for i in range(batch_codes.shape[0]):
                codes = batch_codes[i]  # [x]
                if not has_warned and codes[-1] != self.stop_mel_token:
                    warnings.warn(
                        f"WARN: generation stopped due to exceeding `max_mel_tokens` ({max_mel_tokens}). "
                        f"Consider reducing `max_text_tokens_per_sentence`({max_text_tokens_per_sentence}) or increasing `max_mel_tokens`.",
                        category=RuntimeWarning
                    )
                    has_warned = True
                codes = codes.unsqueeze(0)  # [x] -> [1, x]
                if verbose:
                    print("codes:", codes.shape)
                    print(codes)
                codes, code_lens = self.remove_long_silence(codes, silent_token=52, max_consecutive=30)
                if verbose:
                    print("fix codes:", codes.shape)
                    print(codes)
                    print("code_lens:", code_lens)
                text_tokens = item_tokens[i]
                original_idx = batch_sentences[i]["idx"]
                
                # Á´ãÂç≥ÁîüÊàêlatentÂíåÈü≥È¢ë
                m_start_time = time.perf_counter()
                with torch.no_grad():
                    with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                        latent = self.gpt(auto_conditioning, text_tokens,
                                        torch.tensor([text_tokens.shape[-1]], device=text_tokens.device), codes,
                                        code_lens*self.gpt.mel_length_compression,
                                        cond_mel_lengths=torch.tensor([auto_conditioning.shape[-1]], device=text_tokens.device),
                                        return_latent=True, clip_inputs=False)
                        gpt_forward_time += time.perf_counter() - m_start_time
                        
                        # Á´ãÂç≥ËøõË°åBigVGANËß£Á†Å
                        m_start_time = time.perf_counter()
                        wav, _ = self.bigvgan(latent, auto_conditioning.transpose(1, 2))
                        bigvgan_time += time.perf_counter() - m_start_time
                        wav = wav.squeeze(1)
                        wav = torch.clamp(32767 * wav, -32767.0, 32767.0)
                        
                        # Â≠òÂÇ®Âà∞ÊúâÂ∫èÂ≠óÂÖ∏‰∏≠ÔºåÁ®çÂêéÊåâÂéüÂßãÈ°∫Â∫èÂêàÂπ∂
                        ordered_wavs[original_idx] = wav.cpu()
                        
                # Á´ãÂç≥Ê∏ÖÁêÜÂΩìÂâçÂ§ÑÁêÜÁöÑÂº†Èáè
                del codes, text_tokens, latent, wav
                
            # Ê∏ÖÁêÜÂΩìÂâçÊâπÊ¨°ÁöÑÊï∞ÊçÆ
            del batch_text_tokens, batch_codes
            
            # ËÆ∞ÂΩïÂΩìÂâçÊâπÊ¨°ÂÆåÊàêÊó∂Èó¥
            current_batch_time = time.perf_counter() - current_batch_start
            batch_times.append(current_batch_time)
            
            # ÂÆöÊúüÊ∏ÖÁêÜGPUÁºìÂ≠ò
            if (batch_idx + 1) % max(1, bucket_max_size // 2) == 0:
                self.comprehensive_memory_cleanup(verbose=verbose)
                if verbose:
                    memory_allocated = torch.cuda.memory_allocated(self.device) / 1024**3 if "cuda" in str(self.device) else 0
                    print(f">> ÊâπÊ¨° {batch_idx+1} Â§ÑÁêÜÂÆåÊàêÔºåÂΩìÂâçÊòæÂ≠òÂç†Áî®: {memory_allocated:.2f}GBÔºåÊâπÊ¨°ËÄóÊó∂: {self.format_time(current_batch_time)}")
            
            batch_idx += 1
        
        # Ê∏ÖÁêÜÂ§ßÂûãÂèòÈáèÈáäÊîæÂÜÖÂ≠ò
        del all_text_tokens, all_sentences
        self.torch_empty_cache()  # ÊâßË°å‰∏ÄÊ¨°Âº∫Âà∂ÂÜÖÂ≠òÊ∏ÖÁêÜ
        
        # ÊåâÂéüÂßãÈ°∫Â∫èÂêàÂπ∂Èü≥È¢ë
        print(">> ÂêàÂπ∂Èü≥È¢ëÁâáÊÆµ...")
        self._set_gr_progress(0.9, "ÂêàÂπ∂Èü≥È¢ëÁâáÊÆµ...", start_time=start_time)
        
        # ÊåâÁ¥¢ÂºïÈ°∫Â∫èÊéíÂ∫èÂπ∂ÂêàÂπ∂
        sorted_indices = sorted(ordered_wavs.keys())
        for idx in sorted_indices:
            wavs.append(ordered_wavs[idx])
        
        # Ê∏ÖÁêÜÊúâÂ∫èÂ≠óÂÖ∏
        del ordered_wavs
        
        end_time = time.perf_counter()
        self.torch_empty_cache()  # ÊúÄÁªàÊ∏ÖÁêÜÊâÄÊúâGPUÁºìÂ≠ò

        # wav audio output
        self._set_gr_progress(0.95, "‰øùÂ≠òÈü≥È¢ëÊñá‰ª∂...", start_time=start_time)
        wav = torch.cat(wavs, dim=1)
        wav_length = wav.shape[-1] / sampling_rate
        ref_audio_length = cond_mel_frame * 256 / sampling_rate
        rtf = (end_time - start_time) / wav_length
        
        print(f">> ÂèÇËÄÉÈü≥È¢ëÈïøÂ∫¶: {ref_audio_length:.2f} Áßí")
        print(f">> GPTÁîüÊàêÊó∂Èó¥: {gpt_gen_time:.2f} Áßí")
        print(f">> GPTÂâçÂêëÊó∂Èó¥: {gpt_forward_time:.2f} Áßí") 
        print(f">> BigVGANËß£Á†ÅÊó∂Èó¥: {bigvgan_time:.2f} Áßí")
        print(f">> ÊÄªÊé®ÁêÜÊó∂Èó¥: {end_time - start_time:.2f} Áßí")
        print(f">> ÁîüÊàêÈü≥È¢ëÈïøÂ∫¶: {wav_length:.2f} Áßí")
        print(f">> ÊâπÊ¨°‰ø°ÊÅØ: {all_batch_num} ‰∏™Âè•Â≠ê, {bucket_count} ‰∏™ÊâπÊ¨°, ÂàÜÊ°∂Â§ßÂ∞è: {bucket_max_size}")
        print(f">> ÂÆûÊó∂Áéá(RTF): {rtf:.4f} ({'Âø´‰∫éÂÆûÊó∂' if rtf < 1.0 else 'ÊÖ¢‰∫éÂÆûÊó∂'})")
        
        # Êõ¥Êñ∞ÊúÄÁªàËøõÂ∫¶
        self._set_gr_progress(0.98, f"Èü≥È¢ëÁîüÊàêÂÆåÊàê - Êó∂Èïø: {wav_length:.1f}Áßí, RTF: {rtf:.3f}", start_time=start_time)
        
        # ÁªìÊùüÊó∂ÂÜçÊ¨°Ê∏ÖÁêÜÂÜÖÂ≠ò
        if verbose:
            print(">> Final memory cleanup...")
        self.torch_empty_cache(verbose=verbose)

        # save audio
        wav = wav.cpu()  # to cpu
        if output_path:
            # Áõ¥Êé•‰øùÂ≠òÈü≥È¢ëÂà∞ÊåáÂÆöË∑ØÂæÑ‰∏≠
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            torchaudio.save(output_path, wav.type(torch.int16), sampling_rate)
            print(">> wav file saved to:", output_path)
            return output_path
        else:
            # ËøîÂõû‰ª•Á¨¶ÂêàGradioÁöÑÊ†ºÂºèË¶ÅÊ±Ç
            wav_data = wav.type(torch.int16)
            wav_data = wav_data.numpy().T
            return (sampling_rate, wav_data)

    # ÂéüÂßãÊé®ÁêÜÊ®°Âºè
    def infer(self, audio_prompt, text, output_path, verbose=False, max_text_tokens_per_sentence=120, **generation_kwargs):
        print(">> start inference...")
        start_time = time.perf_counter()
        self._set_gr_progress(0, "start inference...", start_time=start_time)
        if verbose:
            print(f"origin text:{text}")

        # Â¶ÇÊûúÂèÇËÄÉÈü≥È¢ëÊîπÂèò‰∫ÜÔºåÊâçÈúÄË¶ÅÈáçÊñ∞ÁîüÊàê cond_mel, ÊèêÂçáÈÄüÂ∫¶
        if self.cache_cond_mel is None or self.cache_audio_prompt != audio_prompt:
            audio, sr = torchaudio.load(audio_prompt)
            audio = torch.mean(audio, dim=0, keepdim=True)
            if audio.shape[0] > 1:
                audio = audio[0].unsqueeze(0)
            audio = torchaudio.transforms.Resample(sr, 24000)(audio)
            cond_mel = MelSpectrogramFeatures()(audio).to(self.device)
            cond_mel_frame = cond_mel.shape[-1]
            if verbose:
                print(f"cond_mel shape: {cond_mel.shape}", "dtype:", cond_mel.dtype)

            self.cache_audio_prompt = audio_prompt
            self.cache_cond_mel = cond_mel
        else:
            cond_mel = self.cache_cond_mel
            cond_mel_frame = cond_mel.shape[-1]
            pass

        self._set_gr_progress(0.1, "text processing...", start_time=start_time)
        auto_conditioning = cond_mel
        text_tokens_list = self.tokenizer.tokenize(text)
        sentences = self.tokenizer.split_sentences(text_tokens_list, max_text_tokens_per_sentence)
        if verbose:
            print("text token count:", len(text_tokens_list))
            print("sentences count:", len(sentences))
            print("max_text_tokens_per_sentence:", max_text_tokens_per_sentence)
            print(*sentences, sep="\n")
        do_sample = generation_kwargs.pop("do_sample", True)
        top_p = generation_kwargs.pop("top_p", 0.8)
        top_k = generation_kwargs.pop("top_k", 30)
        temperature = generation_kwargs.pop("temperature", 1.0)
        autoregressive_batch_size = 1
        length_penalty = generation_kwargs.pop("length_penalty", 0.0)
        num_beams = generation_kwargs.pop("num_beams", 3)
        repetition_penalty = generation_kwargs.pop("repetition_penalty", 10.0)
        max_mel_tokens = generation_kwargs.pop("max_mel_tokens", 600)
        sampling_rate = 24000
        # lang = "EN"
        # lang = "ZH"
        wavs = []
        gpt_gen_time = 0
        gpt_forward_time = 0
        bigvgan_time = 0
        progress = 0
        has_warned = False
        for sent in sentences:
            text_tokens = self.tokenizer.convert_tokens_to_ids(sent)
            text_tokens = torch.tensor(text_tokens, dtype=torch.int32, device=self.device).unsqueeze(0)
            # text_tokens = F.pad(text_tokens, (0, 1))  # This may not be necessary.
            # text_tokens = F.pad(text_tokens, (1, 0), value=0)
            # text_tokens = F.pad(text_tokens, (0, 1), value=1)
            if verbose:
                print(text_tokens)
                print(f"text_tokens shape: {text_tokens.shape}, text_tokens type: {text_tokens.dtype}")
                # debug tokenizer
                text_token_syms = self.tokenizer.convert_ids_to_tokens(text_tokens[0].tolist())
                print("text_token_syms is same as sentence tokens", text_token_syms == sent)

            # text_len = torch.IntTensor([text_tokens.size(1)], device=text_tokens.device)
            # print(text_len)
            progress += 1
            self._set_gr_progress(0.2 + 0.4 * (progress-1) / len(sentences), f"gpt inference latent... {progress}/{len(sentences)}", 
                                start_time=start_time, total_items=len(sentences), current_item=progress-1)
            m_start_time = time.perf_counter()
            with torch.no_grad():
                with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    codes = self.gpt.inference_speech(auto_conditioning, text_tokens,
                                                        cond_mel_lengths=torch.tensor([auto_conditioning.shape[-1]],
                                                                                      device=text_tokens.device),
                                                        # text_lengths=text_len,
                                                        do_sample=do_sample,
                                                        top_p=top_p,
                                                        top_k=top_k,
                                                        temperature=temperature,
                                                        num_return_sequences=autoregressive_batch_size,
                                                        length_penalty=length_penalty,
                                                        num_beams=num_beams,
                                                        repetition_penalty=repetition_penalty,
                                                        max_generate_length=max_mel_tokens,
                                                        **generation_kwargs)
                gpt_gen_time += time.perf_counter() - m_start_time
                if not has_warned and (codes[:, -1] != self.stop_mel_token).any():
                    warnings.warn(
                        f"WARN: generation stopped due to exceeding `max_mel_tokens` ({max_mel_tokens}). "
                        f"Input text tokens: {text_tokens.shape[1]}. "
                        f"Consider reducing `max_text_tokens_per_sentence`({max_text_tokens_per_sentence}) or increasing `max_mel_tokens`.",
                        category=RuntimeWarning
                    )
                    has_warned = True

                code_lens = torch.tensor([codes.shape[-1]], device=codes.device, dtype=codes.dtype)
                if verbose:
                    print(codes, type(codes))
                    print(f"codes shape: {codes.shape}, codes type: {codes.dtype}")
                    print(f"code len: {code_lens}")

                # remove ultra-long silence if exits
                # temporarily fix the long silence bug.
                codes, code_lens = self.remove_long_silence(codes, silent_token=52, max_consecutive=30)
                if verbose:
                    print(codes, type(codes))
                    print(f"fix codes shape: {codes.shape}, codes type: {codes.dtype}")
                    print(f"code len: {code_lens}")
                self._set_gr_progress(0.2 + 0.4 * progress / len(sentences), f"gpt inference speech... {progress}/{len(sentences)}", 
                                    start_time=start_time, total_items=len(sentences), current_item=progress)
                m_start_time = time.perf_counter()
                # latent, text_lens_out, code_lens_out = \
                with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    latent = \
                        self.gpt(auto_conditioning, text_tokens,
                                    torch.tensor([text_tokens.shape[-1]], device=text_tokens.device), codes,
                                    code_lens*self.gpt.mel_length_compression,
                                    cond_mel_lengths=torch.tensor([auto_conditioning.shape[-1]], device=text_tokens.device),
                                    return_latent=True, clip_inputs=False)
                    gpt_forward_time += time.perf_counter() - m_start_time

                    m_start_time = time.perf_counter()
                    wav, _ = self.bigvgan(latent, auto_conditioning.transpose(1, 2))
                    bigvgan_time += time.perf_counter() - m_start_time
                    wav = wav.squeeze(1)

                wav = torch.clamp(32767 * wav, -32767.0, 32767.0)
                if verbose:
                    print(f"wav shape: {wav.shape}", "min:", wav.min(), "max:", wav.max())
                # wavs.append(wav[:, :-512])
                wavs.append(wav.cpu())  # to cpu before saving
        end_time = time.perf_counter()
        self._set_gr_progress(0.9, "save audio...", start_time=start_time)
        wav = torch.cat(wavs, dim=1)
        wav_length = wav.shape[-1] / sampling_rate
        print(f">> Reference audio length: {cond_mel_frame * 256 / sampling_rate:.2f} seconds")
        print(f">> gpt_gen_time: {gpt_gen_time:.2f} seconds")
        print(f">> gpt_forward_time: {gpt_forward_time:.2f} seconds")
        print(f">> bigvgan_time: {bigvgan_time:.2f} seconds")
        print(f">> Total inference time: {end_time - start_time:.2f} seconds")
        print(f">> Generated audio length: {wav_length:.2f} seconds")
        print(f">> RTF: {(end_time - start_time) / wav_length:.4f}")

        # save audio
        wav = wav.cpu()  # to cpu
        if output_path:
            # Áõ¥Êé•‰øùÂ≠òÈü≥È¢ëÂà∞ÊåáÂÆöË∑ØÂæÑ‰∏≠
            if os.path.isfile(output_path):
                os.remove(output_path)
                print(">> remove old wav file:", output_path)
            if os.path.dirname(output_path) != "":
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
            torchaudio.save(output_path, wav.type(torch.int16), sampling_rate)
            print(">> wav file saved to:", output_path)
            return output_path
        else:
            # ËøîÂõû‰ª•Á¨¶ÂêàGradioÁöÑÊ†ºÂºèË¶ÅÊ±Ç
            wav_data = wav.type(torch.int16)
            wav_data = wav_data.numpy().T
            return (sampling_rate, wav_data)


if __name__ == "__main__":
    prompt_wav="test_data/input.wav"
    #text="Êôï XUAN4 ÊòØ ‰∏Ä Áßç GAN3 Ëßâ"
    #text='Â§ßÂÆ∂Â•ΩÔºåÊàëÁé∞Âú®Ê≠£Âú®bilibili ‰ΩìÈ™å ai ÁßëÊäÄÔºåËØ¥ÂÆûËØùÔºåÊù•‰πãÂâçÊàëÁªùÂØπÊÉ≥‰∏çÂà∞ÔºÅAIÊäÄÊúØÂ∑≤ÁªèÂèëÂ±ïÂà∞ËøôÊ†∑Âå™Â§∑ÊâÄÊÄùÁöÑÂú∞Ê≠•‰∫ÜÔºÅ'
    text="There is a vehicle arriving in dock number 7?"

    tts = IndexTTS(cfg_path="checkpoints/config.yaml", model_dir="checkpoints", is_fp16=True, use_cuda_kernel=False)
    tts.infer(audio_prompt=prompt_wav, text=text, output_path="gen.wav", verbose=True)
